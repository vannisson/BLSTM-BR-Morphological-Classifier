{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL3CTx6kiUxP",
        "outputId": "bc9e2120-a656-42c7-d8fc-4fcf86e8af5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gAJG9sX6Nway"
      },
      "outputs": [],
      "source": [
        "!pip install -q statsmodels\n",
        "!pip install -q scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wekUNAuML0FY",
        "outputId": "42040c71-e160-4b57-ec89-2d9894a2ff83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Dropout, BatchNormalization, Conv1D, MaxPooling1D, Flatten, Embedding\n",
        "from sklearn.model_selection import KFold\n",
        "import pickle\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A9JDc9p4OF8W"
      },
      "outputs": [],
      "source": [
        "# Load the data from the file\n",
        "\n",
        "with open('/content/drive/MyDrive/MacMorpho/macmorpho-train.txt', 'r') as f:\n",
        "    data = f.readlines()\n",
        "    \n",
        "# Preprocessing the data\n",
        "X, Y = [], []\n",
        "for line in data:\n",
        "    tokens = line.strip().split()\n",
        "    X.append([t.split('_')[0].lower() for t in tokens])\n",
        "    Y.append([t.split('_')[1] for t in tokens])\n",
        "    \n",
        "# Creating vocabulary and dictionaries\n",
        "vocab = sorted(set(np.concatenate(X)))\n",
        "vocab.append('<UNK>')  # add <UNK> to represent unknown words\n",
        "tag_dict = {t: i for i, t in enumerate(sorted(set(np.concatenate(Y))))}\n",
        "reverse_tag_dict = {v: k for k, v in tag_dict.items()}\n",
        "word_dict = {w: i+1 for i, w in enumerate(vocab)}  # shift indices by 1 to make room for <UNK>\n",
        "word_dict['<UNK>'] = 0  # add <UNK> to dictionary and give it index 0\n",
        "\n",
        "# Converting words and tags to numbers\n",
        "X = [[word_dict[word] for word in sent] for sent in X]\n",
        "Y = [[tag_dict[tag] for tag in sent] for sent in Y]\n",
        "\n",
        "# Padding the sequences\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X)\n",
        "Y = tf.keras.preprocessing.sequence.pad_sequences(Y)\n",
        "\n",
        "# # Creating train and validation sets\n",
        "# X_train, X_val = X[:50000], X[50000:]\n",
        "# Y_train, Y_val = Y[:50000], Y[50000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy7c1aCWOMyN",
        "outputId": "66d9bb24-8271-482f-a44e-ed467e75a5aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "238/238 [==============================] - 348s 1s/step - loss: 1.6290 - accuracy: 0.5218 - val_loss: 0.4456 - val_accuracy: 0.8753\n",
            "Epoch 2/10\n",
            "238/238 [==============================] - 307s 1s/step - loss: 0.4414 - accuracy: 0.8814 - val_loss: 0.2744 - val_accuracy: 0.9214\n",
            "Epoch 3/10\n",
            "238/238 [==============================] - 302s 1s/step - loss: 0.2834 - accuracy: 0.9268 - val_loss: 0.2295 - val_accuracy: 0.9321\n",
            "Epoch 4/10\n",
            "238/238 [==============================] - 301s 1s/step - loss: 0.2221 - accuracy: 0.9421 - val_loss: 0.2275 - val_accuracy: 0.9368\n",
            "Epoch 5/10\n",
            "238/238 [==============================] - 300s 1s/step - loss: 0.1881 - accuracy: 0.9505 - val_loss: 0.2176 - val_accuracy: 0.9389\n",
            "Epoch 6/10\n",
            "238/238 [==============================] - 297s 1s/step - loss: 0.1635 - accuracy: 0.9569 - val_loss: 0.2339 - val_accuracy: 0.9393\n",
            "Epoch 7/10\n",
            "238/238 [==============================] - 297s 1s/step - loss: 0.1454 - accuracy: 0.9613 - val_loss: 0.2366 - val_accuracy: 0.9404\n",
            "Epoch 8/10\n",
            "238/238 [==============================] - 297s 1s/step - loss: 0.1317 - accuracy: 0.9646 - val_loss: 0.2496 - val_accuracy: 0.9415\n",
            "Epoch 9/10\n",
            "238/238 [==============================] - 299s 1s/step - loss: 0.1195 - accuracy: 0.9679 - val_loss: 0.2637 - val_accuracy: 0.9403\n",
            "Epoch 10/10\n",
            "238/238 [==============================] - 294s 1s/step - loss: 0.1097 - accuracy: 0.9702 - val_loss: 0.2636 - val_accuracy: 0.9410\n",
            "60/60 [==============================] - 9s 142ms/step - loss: 0.2636 - accuracy: 0.9410\n",
            "Epoch 1/10\n",
            "238/238 [==============================] - 331s 1s/step - loss: 1.5973 - accuracy: 0.5292 - val_loss: 0.4354 - val_accuracy: 0.8798\n",
            "Epoch 2/10\n",
            "238/238 [==============================] - 303s 1s/step - loss: 0.4340 - accuracy: 0.8857 - val_loss: 0.2725 - val_accuracy: 0.9232\n",
            "Epoch 3/10\n",
            "238/238 [==============================] - 299s 1s/step - loss: 0.2835 - accuracy: 0.9272 - val_loss: 0.2321 - val_accuracy: 0.9325\n",
            "Epoch 4/10\n",
            "238/238 [==============================] - 302s 1s/step - loss: 0.2254 - accuracy: 0.9416 - val_loss: 0.2204 - val_accuracy: 0.9362\n",
            "Epoch 5/10\n",
            "238/238 [==============================] - 297s 1s/step - loss: 0.1933 - accuracy: 0.9497 - val_loss: 0.2330 - val_accuracy: 0.9367\n",
            "Epoch 6/10\n",
            "238/238 [==============================] - 296s 1s/step - loss: 0.1682 - accuracy: 0.9555 - val_loss: 0.2241 - val_accuracy: 0.9397\n",
            "Epoch 7/10\n",
            "238/238 [==============================] - 295s 1s/step - loss: 0.1487 - accuracy: 0.9603 - val_loss: 0.2295 - val_accuracy: 0.9412\n",
            "Epoch 8/10\n",
            "238/238 [==============================] - 297s 1s/step - loss: 0.1341 - accuracy: 0.9642 - val_loss: 0.2281 - val_accuracy: 0.9433\n",
            "Epoch 9/10\n",
            "238/238 [==============================] - 295s 1s/step - loss: 0.1222 - accuracy: 0.9671 - val_loss: 0.2289 - val_accuracy: 0.9433\n",
            "Epoch 10/10\n",
            "238/238 [==============================] - 295s 1s/step - loss: 0.1116 - accuracy: 0.9699 - val_loss: 0.2624 - val_accuracy: 0.9425\n",
            "60/60 [==============================] - 9s 144ms/step - loss: 0.2624 - accuracy: 0.9425\n",
            "Epoch 1/10\n",
            "238/238 [==============================] - 332s 1s/step - loss: 1.6634 - accuracy: 0.5112 - val_loss: 0.4910 - val_accuracy: 0.8611\n",
            "Epoch 2/10\n",
            "238/238 [==============================] - 304s 1s/step - loss: 0.4532 - accuracy: 0.8819 - val_loss: 0.2752 - val_accuracy: 0.9231\n",
            "Epoch 3/10\n",
            "238/238 [==============================] - 298s 1s/step - loss: 0.2808 - accuracy: 0.9285 - val_loss: 0.2419 - val_accuracy: 0.9315\n",
            "Epoch 4/10\n",
            "238/238 [==============================] - 298s 1s/step - loss: 0.2197 - accuracy: 0.9434 - val_loss: 0.2270 - val_accuracy: 0.9360\n",
            "Epoch 5/10\n",
            "238/238 [==============================] - 296s 1s/step - loss: 0.1855 - accuracy: 0.9517 - val_loss: 0.2315 - val_accuracy: 0.9375\n",
            "Epoch 6/10\n",
            "238/238 [==============================] - 295s 1s/step - loss: 0.1607 - accuracy: 0.9579 - val_loss: 0.2235 - val_accuracy: 0.9401\n",
            "Epoch 7/10\n",
            "238/238 [==============================] - 297s 1s/step - loss: 0.1425 - accuracy: 0.9621 - val_loss: 0.2304 - val_accuracy: 0.9409\n",
            "Epoch 8/10\n",
            "238/238 [==============================] - 295s 1s/step - loss: 0.1281 - accuracy: 0.9661 - val_loss: 0.2424 - val_accuracy: 0.9410\n",
            "Epoch 9/10\n",
            "238/238 [==============================] - 292s 1s/step - loss: 0.1170 - accuracy: 0.9689 - val_loss: 0.2506 - val_accuracy: 0.9416\n",
            "Epoch 10/10\n",
            "238/238 [==============================] - 294s 1s/step - loss: 0.1076 - accuracy: 0.9712 - val_loss: 0.2487 - val_accuracy: 0.9425\n",
            "60/60 [==============================] - 9s 148ms/step - loss: 0.2487 - accuracy: 0.9425\n",
            "Epoch 1/10\n",
            "238/238 [==============================] - 332s 1s/step - loss: 1.6256 - accuracy: 0.5210 - val_loss: 0.5618 - val_accuracy: 0.8451\n",
            "Epoch 2/10\n",
            "238/238 [==============================] - 300s 1s/step - loss: 0.4339 - accuracy: 0.8853 - val_loss: 0.3783 - val_accuracy: 0.8961\n",
            "Epoch 3/10\n",
            "238/238 [==============================] - 298s 1s/step - loss: 0.2767 - accuracy: 0.9298 - val_loss: 0.3312 - val_accuracy: 0.9084\n",
            "Epoch 4/10\n",
            "238/238 [==============================] - 295s 1s/step - loss: 0.2224 - accuracy: 0.9434 - val_loss: 0.3314 - val_accuracy: 0.9103\n",
            "Epoch 5/10\n",
            "238/238 [==============================] - 295s 1s/step - loss: 0.1887 - accuracy: 0.9516 - val_loss: 0.3266 - val_accuracy: 0.9126\n",
            "Epoch 6/10\n",
            "238/238 [==============================] - 295s 1s/step - loss: 0.1636 - accuracy: 0.9576 - val_loss: 0.3439 - val_accuracy: 0.9121\n",
            "Epoch 7/10\n",
            "238/238 [==============================] - 294s 1s/step - loss: 0.1444 - accuracy: 0.9625 - val_loss: 0.3379 - val_accuracy: 0.9158\n",
            "Epoch 8/10\n",
            "238/238 [==============================] - 293s 1s/step - loss: 0.1288 - accuracy: 0.9661 - val_loss: 0.3563 - val_accuracy: 0.9158\n",
            "Epoch 9/10\n",
            "238/238 [==============================] - 292s 1s/step - loss: 0.1158 - accuracy: 0.9693 - val_loss: 0.3546 - val_accuracy: 0.9165\n",
            "Epoch 10/10\n",
            "238/238 [==============================] - 291s 1s/step - loss: 0.1069 - accuracy: 0.9715 - val_loss: 0.3589 - val_accuracy: 0.9179\n",
            "60/60 [==============================] - 9s 142ms/step - loss: 0.3589 - accuracy: 0.9179\n",
            "Epoch 1/10\n",
            "238/238 [==============================] - 326s 1s/step - loss: 1.5776 - accuracy: 0.5365 - val_loss: 0.6136 - val_accuracy: 0.8422\n",
            "Epoch 2/10\n",
            "238/238 [==============================] - 296s 1s/step - loss: 0.3993 - accuracy: 0.8960 - val_loss: 0.4345 - val_accuracy: 0.8837\n",
            "Epoch 3/10\n",
            "238/238 [==============================] - 294s 1s/step - loss: 0.2590 - accuracy: 0.9346 - val_loss: 0.3944 - val_accuracy: 0.8943\n",
            "Epoch 4/10\n",
            "238/238 [==============================] - 292s 1s/step - loss: 0.2055 - accuracy: 0.9473 - val_loss: 0.3889 - val_accuracy: 0.8999\n",
            "Epoch 5/10\n",
            "238/238 [==============================] - 291s 1s/step - loss: 0.1732 - accuracy: 0.9552 - val_loss: 0.3900 - val_accuracy: 0.9025\n",
            "Epoch 6/10\n",
            "238/238 [==============================] - 291s 1s/step - loss: 0.1520 - accuracy: 0.9605 - val_loss: 0.3824 - val_accuracy: 0.9043\n",
            "Epoch 7/10\n",
            "238/238 [==============================] - 291s 1s/step - loss: 0.1340 - accuracy: 0.9647 - val_loss: 0.3894 - val_accuracy: 0.9055\n",
            "Epoch 8/10\n",
            "238/238 [==============================] - 290s 1s/step - loss: 0.1214 - accuracy: 0.9682 - val_loss: 0.4231 - val_accuracy: 0.9055\n",
            "Epoch 9/10\n",
            "238/238 [==============================] - 291s 1s/step - loss: 0.1099 - accuracy: 0.9710 - val_loss: 0.4282 - val_accuracy: 0.9067\n",
            "Epoch 10/10\n",
            "238/238 [==============================] - 290s 1s/step - loss: 0.1024 - accuracy: 0.9727 - val_loss: 0.4527 - val_accuracy: 0.9052\n",
            "60/60 [==============================] - 8s 140ms/step - loss: 0.4527 - accuracy: 0.9052\n"
          ]
        }
      ],
      "source": [
        "# Set the number of folds\n",
        "n_splits = 5\n",
        "\n",
        "# Initialize the KFold object\n",
        "kf = KFold(n_splits=n_splits)\n",
        "\n",
        "# Create an empty list to store the validation accuracie, AICs and models\n",
        "val_accuracies = []\n",
        "val_aics = []\n",
        "models = []\n",
        "\n",
        "\n",
        "# Loop over the folds\n",
        "for train_index, val_index in kf.split(X):\n",
        "    # Split the data into train and validation sets\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
        "\n",
        "    # Define the model architecture\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(vocab), output_dim=128, mask_zero=True))\n",
        "    model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
        "    model.add(Dropout(0.7))\n",
        "    model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
        "    model.add(Dropout(0.7))\n",
        "    model.add(Dense(units=len(tag_dict), activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, batch_size=128)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    _, accuracy = model.evaluate(X_val, Y_val, batch_size=128)\n",
        "\n",
        "    # Calculate the AIC for the model\n",
        "    k = model.count_params()\n",
        "    L = np.exp(-accuracy / 2)\n",
        "    aic = 2 * k - 2 * np.log(L)\n",
        "\n",
        "    # Append the validation accuracy and AIC to the lists\n",
        "    val_accuracies.append(accuracy)\n",
        "    val_aics.append(aic)\n",
        "    models.append(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpEC13jWOS6P",
        "outputId": "4af7c135-cba7-4f0d-b7c8-e07dc5d9f42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy: 0.9298357963562012\n"
          ]
        }
      ],
      "source": [
        "# Calculate the average validation accuracy\n",
        "avg_val_accuracy = np.mean(val_accuracies)\n",
        "\n",
        "# Print the average validation accuracy\n",
        "print('Average validation accuracy:', avg_val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "249OdLy9OXY_",
        "outputId": "f9f81c54-c1ba-4c23-bdfc-026ee250bce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo Salvo!\n",
            "79/79 [==============================] - 16s 196ms/step - loss: 0.2739 - accuracy: 0.9414\n",
            "Test Loss: 0.2738824784755707\n",
            "Test Accuracy: 0.9413623213768005\n"
          ]
        }
      ],
      "source": [
        "# Choose the model with the lowest AIC\n",
        "best_model_index = np.argmin(val_aics)\n",
        "best_model = models[best_model_index]\n",
        "\n",
        "# Salva o modelo escolhido em um arquivo\n",
        "with open('/content/drive/MyDrive/MacMorpho/modelo_treinado.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model, f)\n",
        "print(\"Modelo Salvo!\")\n",
        "# Preprocess test data\n",
        "with open('/content/drive/MyDrive/MacMorpho/macmorpho-test.txt', 'r') as f:\n",
        "    test_data = f.readlines()\n",
        "\n",
        "test_X, test_Y = [], []\n",
        "for line in test_data:\n",
        "    tokens = line.strip().split()\n",
        "    test_X.append([word_dict.get(t.split('_')[0].lower(), 0) for t in tokens])  # replace unknown words with <UNK>\n",
        "    test_Y.append([tag_dict[t.split('_')[1]] for t in tokens])\n",
        "\n",
        "# Padding the sequences\n",
        "test_X = tf.keras.preprocessing.sequence.pad_sequences(test_X)\n",
        "test_Y = tf.keras.preprocessing.sequence.pad_sequences(test_Y)\n",
        "\n",
        "# Evaluating the model on test data\n",
        "loss, accuracy = model.evaluate(test_X, test_Y, batch_size=128)\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsDj93_rOY5u",
        "outputId": "5d51df80-78f7-47eb-a4d0-2126f7e422a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            "Era V\n",
            "uma ART\n",
            "vez N\n",
            "um ART\n",
            "rapaz N\n",
            "chamado PCP\n",
            "Ivan NPROP\n",
            ". PU\n",
            "Num PREP+ART\n",
            "certo ADJ\n",
            "dia N\n",
            "o ART\n",
            "Ivan NPROP\n",
            "foi V\n",
            "à PREP+ART\n",
            "escola N\n",
            "ele PROPESS\n",
            "era V\n",
            "tão ADV\n",
            "distraído, ADV\n",
            "que KS\n",
            "precisava V\n",
            "dos PREP+ART\n",
            "professores. ADV\n",
            "Passado N\n",
            "algum PROADJ\n",
            "tempo N\n",
            "chegou V\n",
            "a ART\n",
            "hora N\n",
            "do PREP+ART\n",
            "lanche, ADV\n",
            "o ART\n",
            "Ivan NPROP\n",
            "enquanto KS\n",
            "lanchava ADV\n",
            "imitava ADV\n",
            "as ART\n",
            "pessoas N\n",
            "quando KS\n",
            "tocou V\n",
            "para PREP\n",
            "o PROSUB\n",
            "intervalo. ADV\n",
            "Como KS\n",
            "estava V\n",
            "um ART\n",
            "dia N\n",
            "de PREP\n",
            "sol N\n",
            "o ART\n",
            "Ivan NPROP\n",
            "foi V\n",
            "jogar V\n",
            "futebol N\n",
            "como KS\n",
            "ele PROPESS\n",
            "era V\n",
            "um PROSUB\n",
            "dos PREP+ART\n",
            "melhores ADJ\n",
            "a PREP\n",
            "jogar V\n",
            "estava V\n",
            "na PREP+ART\n",
            "equipe N\n",
            "principal ADJ\n",
            "da PREP+ART\n",
            "escola N\n"
          ]
        }
      ],
      "source": [
        "# Exemplo de frase para classificar\n",
        "input_text = \"Era uma vez um rapaz chamado Ivan . Num certo dia o Ivan foi à escola ele era tão distraído, que precisava dos professores. Passado algum tempo chegou a hora do lanche, o Ivan enquanto lanchava imitava as pessoas quando tocou para o intervalo. Como estava um dia de sol o Ivan foi jogar futebol como ele era um dos melhores a jogar estava na equipe principal da escola\"\n",
        "\n",
        "# Converte a frase em uma sequência de índices de palavras\n",
        "input_sequence = [word_dict.get(word.lower(), 0) for word in input_text.split()]\n",
        "\n",
        "# Adiciona padding à sequência\n",
        "padded_input_sequence = tf.keras.preprocessing.sequence.pad_sequences([input_sequence])\n",
        "\n",
        "# Faz a previsão das etiquetas das palavras na frase\n",
        "predicted_tags = model.predict(padded_input_sequence)[0]\n",
        "\n",
        "# Converte os índices das etiquetas em suas representações em texto\n",
        "predicted_tags_text = [reverse_tag_dict[np.argmax(tag)] for tag in predicted_tags]\n",
        "\n",
        "# Imprime as palavras e suas etiquetas previstas\n",
        "for i, word in enumerate(input_text.split()):\n",
        "    print(word, predicted_tags_text[i])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}